{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd6c46eb-33a9-41e7-a4b7-2c68828faec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "#import matplotlib.axes as axes\n",
    "import re\n",
    "import os\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ae5264d-b6ee-480e-949e-aea8fd026355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   grdc_no        river                 station  name_in Pegelonline\n",
      "0  6335050  RHINE RIVER             DUESSELDORF            Dsseldorf\n",
      "1  6340180   ELBE RIVER  MAGDEBURG-STROMBRUECKE  magdeburgstrombrcke\n",
      "           6335050     6340180\n",
      "0      1880.000000  397.000000\n",
      "1      2120.000000  388.000000\n",
      "2      2080.000000  376.000000\n",
      "3      1960.000000  383.000000\n",
      "4      1740.000000  389.000000\n",
      "...            ...         ...\n",
      "45518  1245.479167  157.041667\n",
      "45519  1189.291667  153.156250\n",
      "45520  1155.645833  147.541667\n",
      "45521  1124.395833  141.895833\n",
      "45522  1106.071429  140.341463\n",
      "\n",
      "[45523 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "### FLÜSSE UND MESSSTATIONEN AUSLESEN UND SORTIEREN\n",
    "\n",
    "# einen leeren DataFrame erstellen und die Datei einlesen\n",
    "df_rv=pd.DataFrame()\n",
    "dateipfad = 'm_2.txt'\n",
    "#dateipfad = 'm_all_1.csv'\n",
    "data=pd.read_csv(dateipfad, sep=';')\n",
    "data.sort_values(by='grdc_no').reset_index() \n",
    "print(data)\n",
    "\n",
    "# erstellt eine Liste mit den Messstellen-IDs als String\n",
    "mst_valid_ids = data['grdc_no'].tolist()\n",
    "mst_valid_lds=[str(i) for i in mst_valid_ids]\n",
    "\n",
    "# erstellt eine Liste mit den Flussnamen als String\n",
    "river_ids=data['river'].tolist()\n",
    "river_ids=[str(i) for i in river_ids]\n",
    "\n",
    "# erstellt eine Liste mit den Stationsnamen\n",
    "label_ids=data['station'].tolist()\n",
    "\n",
    "# Speichern des DataFrames \"data\" unter neuen Namen (quasi eine Kopie erstellen)\n",
    "df_ids=data\n",
    "df_rv_ids=data\n",
    "#print(df_rv_ids)\n",
    "\n",
    "# # Spalte 't_start' & 't_end' aus dem DataFrame 'df_rv_ids' in DateTime-Objekte vom Format YYYY-MM-DD umwandeln\n",
    "# df_rv_ids['t_start'] = pd.to_datetime(df_rv_ids['t_start'])\n",
    "# #df_rv_ids['t_start'] = df_rv_ids['t_start'].dt.strftime(\"%Y-%m-%d\")\n",
    "# df_rv_ids['t_end']   = pd.to_datetime(df_rv_ids['t_end'])\n",
    "# #df_rv_ids['t_end'] = df_rv_ids['t_end'].dt.strftime(\"%Y-%m-%d\")\n",
    "# #print(df_rv_ids)\n",
    "\n",
    "# # aus den Start- & Endwerten nur das Jahr extrahieren und in einer neuen Spalte speichern\n",
    "# df_rv_ids['y_start'] = df_rv_ids['t_start'].dt.year\n",
    "# df_rv_ids['y_end']   = df_rv_ids['t_end'].dt.year\n",
    "\n",
    "for mst_id in mst_valid_ids:\n",
    "    try:\n",
    "        # aufrufen aller Dateien mit den unterschiedlichen Messstellen-IDs\n",
    "        path= f'{mst_id}_monthly.txt'\n",
    "        data = pd.read_csv(path, sep=';')\n",
    "        # Die Werte der Spalte 'wasserstd_m' extrahieren und in 'df_rv' packen\n",
    "        df_rv[mst_id] = data['wasserstd_m'] #.tolist()[0:45474]\n",
    "    except Exception as e:\n",
    "        print(f'{mst_id} no found!')\n",
    "\n",
    "print(df_rv)\n",
    "# print(df_rv_ids)\n",
    "# print(river_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e4a718-9e0b-4065-bf07-36435257eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           6335050     6340180\n",
      "0      1167.022917  311.000000\n",
      "1      1166.925000  310.600000\n",
      "2      1198.000000  311.200000\n",
      "3      1300.000000  300.800000\n",
      "4      1383.208333  316.000000\n",
      "...            ...         ...\n",
      "45518  1298.445833  211.000000\n",
      "45519  1288.000000  215.016667\n",
      "45520  1304.000000  215.545833\n",
      "45521  1292.000000  213.827083\n",
      "45522  1285.968750  213.991667\n",
      "\n",
      "[45523 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "### VARIABLER THRESHOLD\n",
    "\n",
    "threshold=80\n",
    "p_threshold=(100-threshold)/100\n",
    "\n",
    "# leerer DataFrame\n",
    "df_mt_rv=pd.DataFrame()\n",
    "plot=False\n",
    "\n",
    "# für jede Spalte im DataFrame df_rv -> jeder Fluss mit dazugehörigen Werten\n",
    "for j in df_rv.columns:\n",
    "    # Datenreihe über die gesamte Länge von df_rv (periods), mit Tagen ('D') als Zeitschritte\n",
    "    months = pd.date_range(start='1901-01-01', periods=len(df_rv), freq='D')\n",
    "    # nur die Abflusswerte aus der aktuellen Spalte j aus df_rv als Liste\n",
    "    data=df_rv[j].tolist()\n",
    "    #print(len(data))\n",
    "    # neuer DataFrame, wo wir die Datumsreihe mit den Werten aus df_rv kombinieren\n",
    "    df = pd.DataFrame({'Date': months, 'Value': data})\n",
    "    # Datumsspalte als Index setzen\n",
    "    df['Date']=pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    # das Jahr und der Tag des Jahres wird angegeben (aber doppelt, warum?)\n",
    "    df['D'] = df.index.dayofyear\n",
    "    df['day_of_year'] = df.index.dayofyear\n",
    "    #print(df[df['D']==1])\n",
    "    #print(df.index.dayofyear)\n",
    "\n",
    "    # Berechnung des 80. Perzentils für jeden Monat\n",
    "\n",
    "    # alle NaN-Werte durch -999 ersetzen und dann löschen\n",
    "    df['Value'] = df['Value'].fillna(-999) # diese Zeile hab ich aus '01_fest_threshold_method' übernommen\n",
    "    df_valid=df[df['Value']!=-999]\n",
    "    #print(df[df['Value']==-999])\n",
    "    #print(df_valid.dtypes)\n",
    "\n",
    "    # Daten in df tageweise gruppieren und dann davon das 80. Quantil bestimmen (quantile(p_threshold))\n",
    "    mt = df_valid.groupby('D')['Value'].quantile(p_threshold)\n",
    "    # Berechnung vom Minimum und Maximum in jeder Gruppe 'D' -> wird jeweils in einer Liste gespeichert\n",
    "    list_b=df_valid.groupby('D')['Value'].min()\n",
    "    list_a=df_valid.groupby('D')['Value'].max()\n",
    "    # umwandeln alle Quantil-Werte 'mt' in eine Liste\n",
    "    mt=mt.tolist()\n",
    "    #print(len(mt))\n",
    "    # erstellen von neuem DataFrame mit den Spalten 'day_of_year' und 'threshold'\n",
    "    threshold_df = pd.DataFrame({'day_of_year': np.arange(1, len(mt)+1), 'threshold': mt})\n",
    "    # zusammenführen von den beiden DataFrames 'threshold_df' und 'df' anhand der Spalte 'day_of_year'\n",
    "    df = df.merge(threshold_df, on='day_of_year', how='left')\n",
    "    #print(df)\n",
    "\n",
    "    # alle Schwellenwerte mit 31 multiplizieren und die Liste 'mt' auf die ersten 366 Elemente begrenzen\n",
    "    mt=mt*31\n",
    "    mt=mt[0:366]\n",
    "    #print(river_ids[mst_valid_ids.index(j)])\n",
    "\n",
    "    # im DataFrame 'df_mt_rv' eine neue Spalte mit dem Namen der aktuellen Station [j] erstellen und mit dem Wert des Thresholds füllen\n",
    "    df_mt_rv[j]=df['threshold']\n",
    "\n",
    "print(df_mt_rv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55e6d19a-2392-470e-8acc-ca9a3b949236",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DÜRRE BERECHNEN\n",
    "\n",
    "import datetime\n",
    "\n",
    "def calculate_drought(q_datas,MT,CDR):   # MT = DataFrame mit den Thresholds, CDR = true/false\n",
    "    df=pd.DataFrame()\n",
    "    #Listen, die die Eigenschaften der Dürreereignisse speichern\n",
    "    ID=[]\n",
    "    events=[]#0,1,2,3\n",
    "    duration=[]#unit=month\n",
    "    mean_deficit=[]#mean_deficit kleiner als 0\n",
    "    max_deficit=[]\n",
    "    sum_deficit=[]\n",
    "    start_time=[]\n",
    "    end_time=[]\n",
    "    max_time=[]   # Tag des größten Defizits einer Dürre\n",
    "    deficit_list=[]\n",
    "    #duration_list=[]\n",
    "    deta=[]\n",
    "    Qs=[]\n",
    "\n",
    "    # die Daten aus q_datas in eine Liste überführen\n",
    "    amean=q_datas.tolist()\n",
    "    #print(amean)\n",
    "    \n",
    "    # Datenreihe über die gesamte Länge von 'amean' (periods), mit Tagen ('D') als Zeitschritte\n",
    "    days = pd.date_range(start='1901-01-01', periods=len(amean), freq='D')\n",
    "    # neuer DataFrame 'df_data' mit den Spalten 'Datum' und 'Value'\n",
    "    df_data = pd.DataFrame({'Date': days, 'Value': amean})\n",
    "    # Datumsspalte als Index in 'df_data' setzen\n",
    "    df_data['Date']=pd.to_datetime(df_data['Date'])\n",
    "    df_data.set_index('Date', inplace=True)\n",
    "\n",
    "    # drought flag\n",
    "    dflag = False \n",
    "    i = 0\n",
    "    index_a = 0\n",
    "    # Fehlwerte (-999) durch einen sehr hohen Wert (999999) ersetzen, damit sie nicht als Dürre erkannt werden\n",
    "    amean=[999999 if int(i) ==-999 else i for i in  amean]\n",
    "    s_time = 0  # Startzeitpunkt des Dürreereignisses\n",
    "    d = 0  # Dauer des Dürreereignisses\n",
    "\n",
    "    # Prüfen von jedem Tag in 'amean'\n",
    "    for data in amean:\n",
    "        \n",
    "        MNQ95 = MT[index_a]  # aus dem DataFrame mit dem Threshold den Wert mit dem index_a raussuchen\n",
    "        \n",
    "        if data < MNQ95:\n",
    "            if dflag==False:   # Beginn einer neuen Dürre\n",
    "                dflag=True \n",
    "                ID.append(i)   # der Dürre eine ID geben\n",
    "                i=i+1\n",
    "                deta.append(index_a-(s_time+d))\n",
    "                s_time=index_a   # setzt die Startzeit\n",
    "                start_time.append(s_time)\n",
    "                #print(s_time)\n",
    "                d=1\n",
    "                Q=[]   # der Abfluss wird gespeichert\n",
    "                Q.append(data)\n",
    "                deficit=[]\n",
    "                deficit.append(MNQ95-data)\n",
    "            elif dflag==True: # während der Dürre\n",
    "                # den aktuellen Abfluss und das Defizit speichern\n",
    "                deficit.append(MNQ95-data)\n",
    "                Q.append(data)\n",
    "                d=d+1\n",
    "                \n",
    "        elif data >= MNQ95:\n",
    "            if dflag==True:   \n",
    "                # Ende einer Dürre\n",
    "                duration.append(d)   # Dauer der Dürre wird in der Liste 'duration' gespeichert\n",
    "                mean_deficit.append(sum(deficit)/len(deficit))   # das durchschnittliche Defizit wird berechnet und wird in der Liste 'mean_deficit' gespeichert\n",
    "                sum_deficit.append(sum(deficit))   # Gesamtsumme der Defizite dieser Dürre wird in der Liste 'sum_deficit' gespeichert\n",
    "                max_deficit.append(max(deficit))   # der maximale Wert des Defizits während der Dürre wird bestimmt und zur Liste 'max_deficit' hinzugefügt\n",
    "                # index_m:\n",
    "                # max(deficit) sucht den maximalen Wert aus der Liste 'deficit' (die Liste beinhaltet immer nur die Werte der aktuellen Dürre)\n",
    "                # deficit.index bestimmt den index von diesem maximalen Wert innerhalb der Liste 'deficit'\n",
    "                # dann wird das zu index_a addiert, aber ich glaube es müsste eigentlich s_time sein\n",
    "                index_m=s_time+deficit.index(max(deficit)) \n",
    "                max_time.append(index_m)   # der berechnete Zeitpunkt von max(deficit) wird zur Liste 'max_time' hinzugefügt\n",
    "                deficit_list.append(deficit)   # die gesamte Liste mit Defiziten wird zur Liste 'deficit_list' hinzugefügt\n",
    "                end_time.append(s_time+d)   # Index des Endzeitpunkts wird zur Liste 'end_time' hinzugefügt\n",
    "                Qs.append(min(Q))   # der minimale Abflusswert wirt zur Liste 'Qs' hinzugefügt\n",
    "                #maxd=-max(deficit)\n",
    "                dflag=False\n",
    "\n",
    "        # für den letzten Tag in unserer Datenreihe, weil der sonst verloren gehen würde wenn er Teil einer Dürre ist\n",
    "        if data == amean[-1]:\n",
    "            if dflag==True:\n",
    "                duration.append(d)\n",
    "                mean_deficit.append(sum(deficit)/len(deficit))\n",
    "                sum_deficit.append(sum(deficit))\n",
    "                max_deficit.append(max(deficit))\n",
    "                index_m=index_a+deficit.index(max(deficit))\n",
    "                max_time.append(index_m)\n",
    "                deficit_list.append(deficit)\n",
    "                end_time.append(s_time+d)\n",
    "                Qs.append(min(Q))\n",
    "                dflag=False\n",
    "\n",
    "        # falls CDR=true, dflag=true und Monatsende: Dürre wird beendet\n",
    "        if CDR == True:\n",
    "            if dflag==True:\n",
    "                if df_data.index.is_month_end[index_a]:\n",
    "                    duration.append(d)\n",
    "                    mean_deficit.append(sum(deficit)/len(deficit))\n",
    "                    sum_deficit.append(sum(deficit))\n",
    "                    max_deficit.append(max(deficit))\n",
    "                    index_m=index_a+deficit.index(max(deficit))\n",
    "                    max_time.append(index_m)\n",
    "                    deficit_list.append(deficit)\n",
    "                    end_time.append(s_time+d)\n",
    "                    Qs.append(min(Q))\n",
    "                    dflag=False\n",
    "\n",
    "        index_a=index_a+1\n",
    "        \n",
    "        \n",
    "    from datetime import date\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "\n",
    "    # Umwandlung des Index in ein richtiges Datum (\"Tage seit 1901-01-01\")\n",
    "    def m2y(m):\n",
    "        startdatum = date(1901, 1, 1)  # Reference start date\n",
    "        neudate = startdatum + relativedelta(days=m)  # Berechnung des neuen Datums durch Hinzufügen von 'm' Tagen zum Startdatum\n",
    "        years = neudate.year - startdatum.year  # Berechnung der Jahre zwischen dem Startdatum und dem neuen Datum\n",
    "        months = neudate.month - startdatum.month  # Berechnung der Differenz in Monaten zwischen den beiden Daten\n",
    "        if months < 0:  # Handle negative months (when crossing a year)\n",
    "            years -= 1\n",
    "            months += 12\n",
    "        years=1901+years   # neues Jahr berechnen\n",
    "        months=months+1   # Monatszahl um eins erhöhen, um sicherzustellen, dass sie im Bereich von 1 bis 12 bleibt (anstatt von Null bis elf)\n",
    "        # Die Funktion gibt drei Werte zurück:\n",
    "            # Anzahl der vergangenen Jahre\n",
    "            # Anzahl der vergangenen Monate,\n",
    "            # neu berechnetes Datum (neudate)\n",
    "        return years, months, neudate  # Return as (years, months)\n",
    "       \n",
    "    # Berechnung der Start- & Enddaten der Dürreereignisse\n",
    "    # für jede Zeitangabe (i) in start_time wird die Funktion m2y aufgerufen\n",
    "    time_months=[m2y(i)[1] for i in start_time]    # der zweite Wert (Monate) aus dem Ergebnis wird zur Liste time_months hinzugefügt\n",
    "    time_years=[m2y(i)[0] for i in start_time]    # der erste Wert (Jahre) geht an 'time_years'\n",
    "    time_date=[m2y(i)[2] for i in start_time]   # ...\n",
    "    end_date=[m2y(i)[2] for i in end_time]\n",
    "\n",
    "    # für jede Zeitangabe (i) in 'max_time' wird erneut die Funktion aufgerufen\n",
    "    time_date_max =[m2y(i)[2] for i in max_time]\n",
    "    time_month_max =[m2y(i)[1] for i in max_time]\n",
    "\n",
    "    # alle Werte aus den Listen in den DataFrame 'df' implementieren\n",
    "    df['ID']=ID\n",
    "    df['duration']=duration\n",
    "    df['time_deta']=deta   #[i if i<=365 else 0 deta]\n",
    "    df['mean_deficit']=mean_deficit\n",
    "    df['max_deficit']=max_deficit\n",
    "    df['sum_deficit']=sum_deficit\n",
    "    df['list_deficit']=deficit_list\n",
    "    df['Qmin']=Qs\n",
    "    df['start_date']=time_date\n",
    "    df['start_year']=time_years\n",
    "    df['start_month']=time_months\n",
    "    df['max_date']=time_date_max\n",
    "    df['max_month']=time_month_max\n",
    "    df['end_date']=end_date\n",
    "    df['start_year']=time_years\n",
    "    df['start_month']=time_months\n",
    "                \n",
    "    return df\n",
    "\n",
    "    \n",
    "#for i in range(len(ID)):\n",
    "    #print('ID=',ID[i])\n",
    "    #print('event type=',events[i])\n",
    "    #print('duration=',duration[i])\n",
    "    #print('mean deficit',mean_deficit[i])\n",
    "    #print('max deficit',max_deficit[i])\n",
    "    #print('start time=',m2y(start_time[i]))\n",
    "    #print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51590035-c15b-4295-a105-af5ebbf08052",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ERGEBNIS AUSGEBEN LASSEN\n",
    "\n",
    "df_result=pd.DataFrame()\n",
    "\n",
    "# die beiden Listen df_rv.columns und river_ids so kombinieren, dass für jede Iteration i den aktuellen Spaltennamen und j die entsprechende Fluss-ID erhält\n",
    "for i,j in zip(df_rv.columns,river_ids):\n",
    "    # ersetzen der NaN-Werte in df_rv durch -999 als int\n",
    "    df_rv[i] = df_rv[i].fillna(-999).astype(int)\n",
    "    # aufrufen der vorher definierten Funktion \"calculate_drought\" für die aktuelle Spalte i\n",
    "    # df_mt_rv[i].tolist(): konvertiert die aktuelle Spalte von 'df_mt_rv' (Thresholds) in eine Liste\n",
    "    # True: bezieht sich auf CDR - eine Dürre wird am Monatsende beendet\n",
    "    df_ssi_c1=calculate_drought(df_rv[i],df_mt_rv[i].tolist(),True)\n",
    "    #df_ssi_c1.to_csv(f'rv_mst_result/VTM_{threshold}/drought_result_test.csv',sep=';')\n",
    "\n",
    "    # erstellen von neuen Spalten im DataFrame 'df_ssi_c1' -> für jede Zeile Name der Station / Fluss\n",
    "    df_ssi_c1['Station']=[i]*len(df_ssi_c1)\n",
    "    df_ssi_c1['River']=[j]*len(df_ssi_c1)\n",
    "    # den DataFrame df_ssi_c1 an das Ende des bereits existierenden DataFrames df_result anhängen\n",
    "    df_result = pd.concat([df_result, df_ssi_c1], ignore_index=True)\n",
    "\n",
    "# Dürreereignisse mit einer Dauer von mindestens 5 Tagen\n",
    "#df_result=df_result[df_result['duration']>=5]\n",
    "\n",
    "# Dürreereignisse mit einem Gesamtdefizit von mindestens 5\n",
    "#df_result=df_result[df_result['sum_deficit']>=5]\n",
    "\n",
    "# eine Kopie von 'df_result' speichern\n",
    "df_result0=df_result\n",
    "# die Datei im zugehörigen Ordner speichern\n",
    "df_result.to_csv(f'rv_mst_result/VTM_{threshold}/drought_result_all.csv',sep=';')\n",
    "\n",
    "# Entfernen von doppelten Fluss-IDs\n",
    "river_id=list(set(river_ids))\n",
    "\n",
    "#print(df_result)\n",
    "#print(df_result0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "675dfc74-bb7a-484b-91e5-02612d6b84fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### STATISTIK\n",
    "\n",
    "# Definition der Variablen a und b durch die Spalten eines DataFrames\n",
    "a='duration'\n",
    "b='max_deficit'\n",
    "# leeren DataFrame erstellen\n",
    "df_result_s=pd.DataFrame()\n",
    "\n",
    "# Schleife über die Spaltennamen [i] in 'df_rv' (IDs der Messstationen) und die Fluss-IDs [j]\n",
    "for i,j in zip(df_rv.columns,river_ids):\n",
    "    # 'df_test' enthält alle Zeilen aus 'df_result' die den aktuellen Stationsnamen [i] enthalten\n",
    "    df_test=df_result[df_result['Station']==i]\n",
    "    x_data1=df_test[f'{a}']\n",
    "    y_data1=df_test[f'{b}']\n",
    "    df_s=pd.DataFrame()\n",
    "    df_s['Station']=[i]\n",
    "    df_s[f'number_of_events']=[len(x_data1)]\n",
    "    df_s[f'mean_max_deficit']=[y_data1.mean()]\n",
    "    df_s[f'mean_duration']=[x_data1.mean()]\n",
    "    df_s[f'max_max_deficit']=[y_data1.max()]\n",
    "    df_s[f'max_duration']=[x_data1.max()]\n",
    "    for season,i in zip(['winter','spring','summer','autumn'],[[11, 12, 1],[2,3, 4],[5, 6, 7],[8, 9, 10]]): \n",
    "        df_result1=df_test[df_test['start_month'].isin(i)]\n",
    "        x_data1=df_result1[f'{a}']\n",
    "        y_data1=df_result1[f'{b}']\n",
    "        df_s[f'{season}_number_of_events']=[len(x_data1)]\n",
    "        df_s[f'{season}_mean_max_deficit']=[y_data1.mean()]\n",
    "        df_s[f'{season}_mean_duration']=[x_data1.mean()]\n",
    "        df_s[f'{season}_max_max_deficit']=[y_data1.max()]\n",
    "        df_s[f'{season}_max_duration']=[x_data1.max()]\n",
    "    df_result_s = pd.concat([df_result_s, df_s], ignore_index=True)\n",
    "\n",
    "# das Ergebnis als CSV speichern - ich habe die Direction in den Ordner VTM_threshold verschoben\n",
    "df_result_s.to_csv(f'rv_mst_result/VTM_{threshold}/drought_result_statistic.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06b99b2d-69e8-4287-840d-dd084f00f7eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'y_start'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m\\\\pazifik.iww.rwth-aachen.de\\projekte\\BMBF_SAFERWATER\\Hiwi\\Flora\\DRYPOWER\\DryPower_Code\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'y_start'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m df_result0=pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mrv_mst_result/VTM_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthreshold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/drought_result_all.csv\u001b[39m\u001b[33m'\u001b[39m,sep=\u001b[33m'\u001b[39m\u001b[33m;\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m statistic=[]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k,j,start_y,end_y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(df_rv.columns,river_ids,\u001b[43mdf_rv_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43my_start\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.tolist(),df_rv_ids[\u001b[33m'\u001b[39m\u001b[33my_end\u001b[39m\u001b[33m'\u001b[39m].tolist()):\n\u001b[32m      5\u001b[39m     df_result_all=pd.DataFrame()\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(start_y)<\u001b[32m1901\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m\\\\pazifik.iww.rwth-aachen.de\\projekte\\BMBF_SAFERWATER\\Hiwi\\Flora\\DRYPOWER\\DryPower_Code\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m\\\\pazifik.iww.rwth-aachen.de\\projekte\\BMBF_SAFERWATER\\Hiwi\\Flora\\DRYPOWER\\DryPower_Code\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'y_start'"
     ]
    }
   ],
   "source": [
    "### ERGEBNISSE NACH SAISONEN\n",
    "\n",
    "df_result0=pd.read_csv(f'rv_mst_result/VTM_{threshold}/drought_result_all.csv',sep=';')\n",
    "statistic=[]\n",
    "\n",
    "for k,j,start_y,end_y in zip(df_rv.columns,river_ids,df_rv_ids['y_start'].tolist(),df_rv_ids['y_end'].tolist()):\n",
    "    df_result_all=pd.DataFrame()\n",
    "    if int(start_y)<1901:\n",
    "        start_y='1901'\n",
    "    years=[i+int(start_y) for i in range(int(end_y)-int(start_y)+1)]\n",
    "    events=[]\n",
    "    df_result=df_result0[df_result0['Station']==k]\n",
    "    \n",
    "    for y in years:\n",
    "        df_result_s=pd.DataFrame()\n",
    "        df_year_1=pd.DataFrame()\n",
    "        df_year_2=pd.DataFrame()\n",
    "        #df_year=df_result0[df_result0['start_year']==y]\n",
    "        df_result_s['year']=[y]\n",
    "        for season,i in zip(['winter','spring','summer','autumn'],[[11, 12, 1],[2,3, 4],[5, 6, 7],[8, 9, 10]]):\n",
    "                df_year_1=df_result[df_result['start_year']==y]\n",
    "                \n",
    "                if season == 'winter':\n",
    "                    df_1=df_year_1[df_year_1['start_month'].isin([1])]\n",
    "                    if y>years[0]:\n",
    "                        df_year_2=df_result[df_result['start_year']==y-1]\n",
    "                        df_2=df_year_2[df_year_2['start_month'].isin([11,12])]\n",
    "                        df_season=pd.concat([df_1, df_2], ignore_index=True)\n",
    "                    else:\n",
    "                        df_season=df_1\n",
    "                else:\n",
    "                    df_season=df_year_1[df_year_1['start_month'].isin(i)]\n",
    "                if len(df_season) ==0:\n",
    "                    df_result_s[f'{season}_cumulative_events']=[0]\n",
    "                    df_result_s[f'{season}_cumulative_duration']=[0]\n",
    "                    df_result_s[f'{season}_mean_duration']=[0]\n",
    "                    df_result_s[f'{season}_cumulative_deficit']=[0]\n",
    "                    df_result_s[f'{season}_max_deficit']=[0]\n",
    "                else:\n",
    "                    df_result_s[f'{season}_cumulative_events']=[1]\n",
    "                    df_result_s[f'{season}_cumulative_duration']=[df_season['duration'].sum()]\n",
    "                    df_result_s[f'{season}_mean_duration']=[df_season['duration'].mean()]\n",
    "                    df_result_s[f'{season}_cumulative_deficit']=[df_season['sum_deficit'].sum()]\n",
    "                    df_result_s[f'{season}_max_deficit']=[df_season['max_deficit'].max()]\n",
    "\n",
    "        df_result_all = pd.concat([df_result_all, df_result_s], ignore_index=True)\n",
    "    #statistic.append({'Station':k})\n",
    "    \n",
    "    statistic.append({\n",
    "        'Station':k,\n",
    "        'River':j,\n",
    "        \n",
    "        f'mean winter_cumulative_events':df_result_all[f'winter_cumulative_events'].mean(),\n",
    "        f'mean spring_cumulative_events':df_result_all[f'spring_cumulative_events'].mean(),\n",
    "        f'mean summer_cumulative_events':df_result_all[f'summer_cumulative_events'].mean(),\n",
    "        f'mean autumn_cumulative_events':df_result_all[f'autumn_cumulative_events'].mean(),\n",
    "        \n",
    "        f'mean winter_cumulative_duration': df_result_all[f'winter_cumulative_duration'].mean(),\n",
    "        f'mean spring_cumulative_duration': df_result_all[f'spring_cumulative_duration'].mean(),\n",
    "        f'mean summer_cumulative_duration': df_result_all[f'summer_cumulative_duration'].mean(),\n",
    "        f'mean autumn_cumulative_duration': df_result_all[f'autumn_cumulative_duration'].mean(),\n",
    "        \n",
    "        f'mean winter_max_deficit': df_result_all[f'winter_max_deficit'].mean(), \n",
    "        f'mean spring_max_deficit': df_result_all[f'spring_max_deficit'].mean(),       \n",
    "        f'mean summer_max_deficit': df_result_all[f'summer_max_deficit'].mean(),\n",
    "        f'mean autumn_max_deficit': df_result_all[f'autumn_max_deficit'].mean(),\n",
    "\n",
    "        f'max winter_max_deficit': df_result_all[f'winter_max_deficit'].max(), \n",
    "        f'max spring_max_deficit': df_result_all[f'spring_max_deficit'].max(),       \n",
    "        f'max summer_max_deficit': df_result_all[f'summer_max_deficit'].max(),\n",
    "        f'max autumn_max_deficit': df_result_all[f'autumn_max_deficit'].max(),\n",
    "\n",
    "        f'max winter_cumulative_duration': df_result_all[f'winter_cumulative_duration'].max(),\n",
    "        f'max spring_cumulative_duration': df_result_all[f'spring_cumulative_duration'].max(),\n",
    "        f'max summer_cumulative_duration': df_result_all[f'summer_cumulative_duration'].max(),\n",
    "        f'max autumn_cumulative_duration': df_result_all[f'autumn_cumulative_duration'].max(),\n",
    "    })\n",
    "    \n",
    "    df_result_all = df_result_all.sort_values(by=\"year\")\n",
    "    df_result_all.to_csv(f'rv_mst_result/VTM_{threshold}/onset_seasonal_drought_result/{j}_{k}.csv',sep=';')\n",
    "statistic=pd.DataFrame(statistic)\n",
    "statistic.to_csv(f'onset_seasonal_drought_statistic.csv',sep=';')\n",
    "#print(statistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd939f8-9319-4977-9456-8c2c7412691b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
